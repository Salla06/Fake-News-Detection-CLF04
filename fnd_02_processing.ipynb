{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection - Phase 2: Data Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "### Objectifs de ce notebook\n",
    "\n",
    "Ce notebook implemente les étapes de preprocessing identifiées lors de l'exploration. Nous suivrons alors la méthodologie décrite dans l'article de référence (Roumeliotis et al., 2025) avec les étapes suivantes:\n",
    "\n",
    "1. Nettoyage des données (valeurs manquantes, doublons)\n",
    "2. Fusion des colonnes titre et texte\n",
    "3. Normalisation du texte\n",
    "4. Tokenisation\n",
    "5. Suppression des stopwords\n",
    "6. Lemmatisation\n",
    "7. Limitation de la longueur du texte\n",
    "9. Division train/validation/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation des bibliotheques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Telechargement des ressources NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des données\n",
    "\n",
    "Nous chargeons le dataset combiné \"combined_raw\" lors de l'exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset chargé: 44898 articles\n",
      "Colonnes: ['title', 'text', 'subject', 'date', 'Detection', 'detect_label', 'text_length_chars', 'text_length_words']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Detection</th>\n",
       "      <th>detect_label</th>\n",
       "      <th>text_length_chars</th>\n",
       "      <th>text_length_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>Fake</td>\n",
       "      <td>2893</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>Fake</td>\n",
       "      <td>1898</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>Fake</td>\n",
       "      <td>3597</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "\n",
       "                date  Detection detect_label  text_length_chars  \\\n",
       "0  December 31, 2017          0         Fake               2893   \n",
       "1  December 31, 2017          0         Fake               1898   \n",
       "2  December 30, 2017          0         Fake               3597   \n",
       "\n",
       "   text_length_words  \n",
       "0                495  \n",
       "1                305  \n",
       "2                580  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chargement du dataset\n",
    "df = pd.read_csv(\"data/combined_raw.csv\", sep=\",\")\n",
    "\n",
    "print(f\"Dataset chargé: {len(df)} articles\")\n",
    "print(f\"Colonnes: {list(df.columns)}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Nettoyage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Gestion des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes avant nettoyage:\n",
      "title                  0\n",
      "text                 631\n",
      "subject                0\n",
      "date                   0\n",
      "Detection              0\n",
      "detect_label           0\n",
      "text_length_chars      0\n",
      "text_length_words      0\n",
      "dtype: int64\n",
      "Nombre d'obs initial : 44898\n"
     ]
    }
   ],
   "source": [
    "# Etat avant nettoyage\n",
    "print(\"Valeurs manquantes avant nettoyage:\")\n",
    "print(df.apply(lambda x: x.astype(str).str.replace(r'\\s+', '', regex=True).eq('').sum()))\n",
    "\n",
    "initial_count = len(df)\n",
    "print(f\"Nombre d'obs initial : {initial_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres d'articles supprimés: 631\n"
     ]
    }
   ],
   "source": [
    "# Supprimer les lignes où text ou Detection ne contiennent que des espaces\n",
    "df = df[~df['text'].astype(str).str.replace(r'\\s+', '', regex=True).eq('')]\n",
    "\n",
    "print(f\"Nombres d'articles supprimés: {initial_count - len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Suppression des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons supprimés: 5623\n",
      "Articles restants: 38644\n"
     ]
    }
   ],
   "source": [
    "initial_count = len(df)\n",
    "df = df.drop_duplicates(subset=['text'], keep='first')\n",
    "\n",
    "print(f\"Doublons supprimés: {initial_count - len(df)}\")\n",
    "print(f\"Articles restants: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fusion des colonnes titre et texte\n",
    "\n",
    "Conformement à la méthodologie de l'article, nous combinons le titre et le texte pour fournir au modéle un contexte complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes 'title' et 'text' fusionnées dans 'combined_text'\n"
     ]
    }
   ],
   "source": [
    "df['combined_text'] = df['title'].astype(str) + \" \" + df['text'].astype(str)\n",
    "print(\"Colonnes 'title' et 'text' fusionnées dans 'combined_text'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing du texte\n",
    "\n",
    "### 5.1 Definition des fonctions de preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des outils NLP\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Conversion en minuscules\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Suppression des URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Suppression des mentions et hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Suppression de la ponctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Suppression des chiffres\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Suppression des espaces multiples\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Nettoyage initial\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Tokenisation\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Filtrage: stopwords et tokens courts\n",
    "    tokens = [token for token in tokens \n",
    "              if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # Lemmatisation\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Reconstruction\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Application du preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original (extrait):\n",
      " Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out\n",
      "\n",
      "Texte preprocesse:\n",
      "donald trump sends embarrassing new year eve message disturbing donald trump wish american happy new year leave instead give shout enemy hater dishonest fake news medium former reality show star one j\n"
     ]
    }
   ],
   "source": [
    "# Test sur un exemple\n",
    "sample_text = df['combined_text'].iloc[0]\n",
    "print(\"Texte original (extrait):\")\n",
    "print(sample_text[:200])\n",
    "print(\"\\nTexte preprocesse:\")\n",
    "print(preprocess_text(sample_text)[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du preprocessing sur tout le dataset\n",
    "df['processed_text'] = df['combined_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparaison avant/apres preprocessing:\n",
      "Original (1824 chars):\n",
      " Racist Alabama Cops Brutalize Black Boy While He Is In Handcuffs (GRAPHIC IMAGES) The number of cases of cops brutalizing and killing people of color\n",
      "Preprocessed (1132 chars):\n",
      "racist alabama cop brutalize black boy handcuff graphic image number case cop brutalizing killing people color seems see end another case need shared \n"
     ]
    }
   ],
   "source": [
    "# Verification des resultats\n",
    "print(\"Comparaison avant/apres preprocessing:\")\n",
    "print(f\"Original ({len(df['combined_text'].iloc[5])} chars):\")\n",
    "print(df['combined_text'].iloc[5][:150])\n",
    "print(f\"Preprocessed ({len(df['processed_text'].iloc[5])} chars):\")\n",
    "print(df['processed_text'].iloc[5][:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Limitation de la longueur du texte\n",
    "\n",
    "Conformement a la methodologie de l'article de reference, nous limitons la longueur du texte a 2560 caracteres pour assurer la compatibilite avec les modeles BERT et CNN qui ont une limite de 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques de longueur avant troncature:\n",
      "count    38644.000000\n",
      "mean      1732.370226\n",
      "std       1332.797909\n",
      "min          0.000000\n",
      "25%        958.000000\n",
      "50%       1560.000000\n",
      "75%       2149.000000\n",
      "max      37847.000000\n",
      "Name: text_length, dtype: float64\n",
      "Textes depassant 2560 caracteres: 6506 (16.8%)\n"
     ]
    }
   ],
   "source": [
    "# Parametre de longueur maximale\n",
    "MAX_LENGTH = 2560\n",
    "\n",
    "# Statistiques avant troncature\n",
    "df['text_length'] = df['processed_text'].apply(len)\n",
    "\n",
    "print(\"Statistiques de longueur avant troncature:\")\n",
    "print(df['text_length'].describe())\n",
    "\n",
    "# Nombre de textes depassant la limite\n",
    "exceeding = (df['text_length'] > MAX_LENGTH).sum()\n",
    "print(f\"Textes depassant {MAX_LENGTH} caracteres: {exceeding} ({exceeding/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques de longueur apres troncature:\n",
      "count    38644.000000\n",
      "mean      1532.362592\n",
      "std        741.310471\n",
      "min          0.000000\n",
      "25%        958.000000\n",
      "50%       1560.000000\n",
      "75%       2149.000000\n",
      "max       2560.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Application de la troncature\n",
    "df['processed_text'] = df['processed_text'].apply(lambda x: x[:MAX_LENGTH])\n",
    "\n",
    "# Mise a jour des longueurs\n",
    "df['text_length'] = df['processed_text'].apply(len)\n",
    "\n",
    "print(\"Statistiques de longueur apres troncature:\")\n",
    "print(df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Suppression des textes vides\n",
    "\n",
    "Après le preprocessing, certains textes peuvent devenir vides. Nous les supprimons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textes vides apres preprocessing: 5\n",
      "Articles supprimes: 5\n",
      "Articles restants: 38,639\n"
     ]
    }
   ],
   "source": [
    "# Verification des textes vides\n",
    "empty_texts = (df['processed_text'].str.strip() == '').sum()\n",
    "print(f\"Textes vides apres preprocessing: {empty_texts}\")\n",
    "\n",
    "# Suppression si necessaire\n",
    "initial_count = len(df)\n",
    "df = df[df['processed_text'].str.strip() != '']\n",
    "\n",
    "print(f\"Articles supprimes: {initial_count - len(df)}\")\n",
    "print(f\"Articles restants: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ajout d'un identifiant unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>Detection</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>detect_label</th>\n",
       "      <th>text_length_chars</th>\n",
       "      <th>text_length_words</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>donald trump sends embarrassing new year eve m...</td>\n",
       "      <td>0</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Fake</td>\n",
       "      <td>2893</td>\n",
       "      <td>495</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>1698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>drunk bragging trump staffer started russian c...</td>\n",
       "      <td>0</td>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Fake</td>\n",
       "      <td>1898</td>\n",
       "      <td>305</td>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>1425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sheriff david clarke becomes internet joke thr...</td>\n",
       "      <td>0</td>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>Fake</td>\n",
       "      <td>3597</td>\n",
       "      <td>580</td>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>2215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>trump obsessed even obama name coded website i...</td>\n",
       "      <td>0</td>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>Fake</td>\n",
       "      <td>2774</td>\n",
       "      <td>444</td>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>1744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>pope francis called donald trump christmas spe...</td>\n",
       "      <td>0</td>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>Fake</td>\n",
       "      <td>2346</td>\n",
       "      <td>420</td>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>1467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                     processed_text  Detection  \\\n",
       "0   0  donald trump sends embarrassing new year eve m...          0   \n",
       "1   1  drunk bragging trump staffer started russian c...          0   \n",
       "2   2  sheriff david clarke becomes internet joke thr...          0   \n",
       "3   3  trump obsessed even obama name coded website i...          0   \n",
       "4   4  pope francis called donald trump christmas spe...          0   \n",
       "\n",
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date detect_label  text_length_chars  text_length_words  \\\n",
       "0  December 31, 2017         Fake               2893                495   \n",
       "1  December 31, 2017         Fake               1898                305   \n",
       "2  December 30, 2017         Fake               3597                580   \n",
       "3  December 29, 2017         Fake               2774                444   \n",
       "4  December 25, 2017         Fake               2346                420   \n",
       "\n",
       "                                       combined_text  text_length  \n",
       "0   Donald Trump Sends Out Embarrassing New Year’...         1698  \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...         1425  \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...         2215  \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...         1744  \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...         1467  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajout de l'identifiant\n",
    "df['id'] = range(len(df))\n",
    "\n",
    "# Reorganisation des colonnes\n",
    "cols = ['id', 'processed_text', 'Detection']\n",
    "additional_cols = [c for c in df.columns if c not in cols]\n",
    "df = df[cols + additional_cols]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Division du dataset\n",
    "\n",
    "Nous divisons le dataset selon le schema de l'article de reference:\n",
    "- **Training set**: 80%\n",
    "- **Validation set**: 20% du training (soit 16% du total)\n",
    "- **Test set**: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division du dataset:\n",
      "  Training set:   24,728 samples (64.0%)\n",
      "  Validation set: 6,183 samples (16.0%)\n",
      "  Test set:       7,728 samples (20.0%)\n"
     ]
    }
   ],
   "source": [
    "# Preparation des données pour la division\n",
    "X = df[['id', 'processed_text']]\n",
    "y = df['Detection']\n",
    "\n",
    "# Premiere division: 80% train+val, 20% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Deuxieme division: 80% train, 20% val (du train+val)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.20, random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(\"Division du dataset:\")\n",
    "print(f\"  Training set:   {len(X_train):,} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Validation set: {len(X_val):,} samples ({len(X_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Test set:       {len(X_test):,} samples ({len(X_test)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution des classes par split:\n",
      "Training set:\n",
      "Detection\n",
      "1    13562\n",
      "0    11166\n",
      "Name: count, dtype: int64\n",
      "Validation set:\n",
      "Detection\n",
      "1    3391\n",
      "0    2792\n",
      "Name: count, dtype: int64\n",
      "Test set:\n",
      "Detection\n",
      "1    4238\n",
      "0    3490\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verification de la distribution des classes dans chaque split\n",
    "print(\"Distribution des classes par split:\")\n",
    "print(\"Training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"Validation set:\")\n",
    "print(y_val.value_counts())\n",
    "print(\"Test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Preparation des datasets finaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets prepares:\n",
      "  train_df: (24728, 4)\n",
      "  val_df:   (6183, 4)\n",
      "  test_df:  (7728, 4)\n"
     ]
    }
   ],
   "source": [
    "# Creation des DataFrames finaux\n",
    "train_df = pd.DataFrame({\n",
    "    'id': X_train['id'].values,\n",
    "    'text': X_train['processed_text'].values,\n",
    "    'label': y_train.values,\n",
    "    'subject': df.loc[X_train.index, 'subject'].values\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'id': X_val['id'].values,\n",
    "    'text': X_val['processed_text'].values,\n",
    "    'label': y_val.values,\n",
    "    'subject' : df.loc[X_val.index, 'subject'].values\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'id': X_test['id'].values,\n",
    "    'text': X_test['processed_text'].values,\n",
    "    'label': y_test.values,\n",
    "    'subject' : df.loc[X_test.index, 'subject'].values\n",
    "})\n",
    "\n",
    "print(\"Datasets prepares:\")\n",
    "print(f\"  train_df: {train_df.shape}\")\n",
    "print(f\"  val_df:   {val_df.shape}\")\n",
    "print(f\"  test_df:  {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sauvegarde des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers sauvegardes:\n",
      "  - ./data/processed/train.csv\n",
      "  - ./data/processed/validation.csv\n",
      "  - ./data/processed/test.csv\n",
      "  - ./data/processed/full_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Creation du repertoire de sortie\n",
    "import os\n",
    "OUTPUT_PATH = \"./data/processed/\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Sauvegarde des fichiers CSV\n",
    "train_df.to_csv(OUTPUT_PATH + \"train.csv\", index=False)\n",
    "val_df.to_csv(OUTPUT_PATH + \"validation.csv\", index=False)\n",
    "test_df.to_csv(OUTPUT_PATH + \"test.csv\", index=False)\n",
    "\n",
    "# Sauvegarde du dataset complet preprocesse\n",
    "df.to_csv(OUTPUT_PATH + \"full_processed.csv\", index=False)\n",
    "\n",
    "print(\"Fichiers sauvegardes:\")\n",
    "print(f\"  - {OUTPUT_PATH}train.csv\")\n",
    "print(f\"  - {OUTPUT_PATH}validation.csv\")\n",
    "print(f\"  - {OUTPUT_PATH}test.csv\")\n",
    "print(f\"  - {OUTPUT_PATH}full_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Résumé du preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESUME DU PREPROCESSING\n",
      "--------------------------------------------------------\n",
      "1. NETTOYAGE DES données\n",
      "   - Colonnes inutiles supprimees\n",
      "   - Valeurs manquantes gerees\n",
      "   - Doublons supprimes\n",
      "2. PREPROCESSING DU TEXTE\n",
      "   - Mise en minuscules\n",
      "   - Suppression URLs, mentions, hashtags\n",
      "   - Suppression ponctuation et chiffres\n",
      "   - Tokenisation\n",
      "   - Suppression des stopwords\n",
      "   - Lemmatisation\n",
      "4. DIVISION DU DATASET\n",
      "   - Training:   24728 samples\n",
      "   - Validation: 6183 samples\n",
      "   - Test:       7728 samples\n",
      "5. FICHIERS GENERES\n",
      "   - ./data/processed/train.csv\n",
      "   - ./data/processed/validation.csv\n",
      "   - ./data/processed/test.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"RESUME DU PREPROCESSING\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "print(\"1. NETTOYAGE DES données\")\n",
    "print(\"   - Colonnes inutiles supprimees\")\n",
    "print(\"   - Valeurs manquantes gerees\")\n",
    "print(\"   - Doublons supprimes\")\n",
    "\n",
    "print(\"2. PREPROCESSING DU TEXTE\")\n",
    "print(\"   - Mise en minuscules\")\n",
    "print(\"   - Suppression URLs, mentions, hashtags\")\n",
    "print(\"   - Suppression ponctuation et chiffres\")\n",
    "print(\"   - Tokenisation\")\n",
    "print(\"   - Suppression des stopwords\")\n",
    "print(\"   - Lemmatisation\")\n",
    "\n",
    "print(\"4. DIVISION DU DATASET\")\n",
    "print(f\"   - Training:   {len(train_df)} samples\")\n",
    "print(f\"   - Validation: {len(val_df)} samples\")\n",
    "print(f\"   - Test:       {len(test_df)} samples\")\n",
    "\n",
    "print(\"5. FICHIERS GENERES\")\n",
    "print(f\"   - {OUTPUT_PATH}train.csv\")\n",
    "print(f\"   - {OUTPUT_PATH}validation.csv\")\n",
    "print(f\"   - {OUTPUT_PATH}test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prochaine étape\n",
    "\n",
    "Le notebook suivant (`fnd_03_modeling_classical.ipynb`) implementera les modeles classiques de classification (TF-IDF + Logistic Regression, Naive Bayes, SVM) comme baseline avant de passer aux modeles de deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- Roumeliotis, K.I., Tselikas, N.D., & Nasiopoulos, D.K. (2025). Fake News Detection and Classification: A Comparative Study of CNNs, LLMs, and NLP Models. *Future Internet*, 17, 28."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
