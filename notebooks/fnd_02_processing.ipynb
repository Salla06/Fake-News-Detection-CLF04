{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection - Phase 2: Data Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "### Objectifs de ce notebook\n",
    "\n",
    "Ce notebook implemente les étapes de preprocessing identifiées lors de l'exploration. Nous suivrons alors la méthodologie décrite dans l'article de référence (Roumeliotis et al., 2025) avec les étapes suivantes:\n",
    "\n",
    "1. Nettoyage des données (valeurs manquantes, doublons)\n",
    "2. Fusion des colonnes titre et texte\n",
    "3. Normalisation du texte\n",
    "4. Tokenisation\n",
    "5. Suppression des stopwords\n",
    "6. Lemmatisation\n",
    "7. Limitation de la longueur du texte\n",
    "9. Division train/validation/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation des bibliotheques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Telechargement des ressources NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins\n",
    "raw = Path('../data/raw')\n",
    "processed = Path('../data/processed')\n",
    "models = Path('../models')\n",
    "\n",
    "# Créer dossiers si nécessaire\n",
    "for path in [raw, processed]:\n",
    "    path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des données\n",
    "\n",
    "Nous chargeons le dataset combiné \"combined_raw\" lors de l'exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset chargé: 44898 articles\n",
      "Colonnes: ['title', 'text', 'subject', 'date', 'Detection', 'detect_label', 'text_length_chars', 'text_length_words']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Detection</th>\n",
       "      <th>detect_label</th>\n",
       "      <th>text_length_chars</th>\n",
       "      <th>text_length_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben Stein Calls Out 9th Circuit Court: Committ...</td>\n",
       "      <td>21st Century Wire says Ben Stein, reputable pr...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>February 13, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>Fake</td>\n",
       "      <td>1028</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump drops Steve Bannon from National Securit...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>April 5, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4820</td>\n",
       "      <td>771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Puerto Rico expects U.S. to lift Jones Act shi...</td>\n",
       "      <td>(Reuters) - Puerto Rico Governor Ricardo Rosse...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>September 27, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1848</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ben Stein Calls Out 9th Circuit Court: Committ...   \n",
       "1  Trump drops Steve Bannon from National Securit...   \n",
       "2  Puerto Rico expects U.S. to lift Jones Act shi...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  21st Century Wire says Ben Stein, reputable pr...       US_News   \n",
       "1  WASHINGTON (Reuters) - U.S. President Donald T...  politicsNews   \n",
       "2  (Reuters) - Puerto Rico Governor Ricardo Rosse...  politicsNews   \n",
       "\n",
       "                  date  Detection detect_label  text_length_chars  \\\n",
       "0    February 13, 2017          0         Fake               1028   \n",
       "1       April 5, 2017           1         True               4820   \n",
       "2  September 27, 2017           1         True               1848   \n",
       "\n",
       "   text_length_words  \n",
       "0                171  \n",
       "1                771  \n",
       "2                304  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chargement du dataset\n",
    "df = pd.read_csv(raw/\"combined_raw.csv\", sep=\",\")\n",
    "\n",
    "print(f\"Dataset chargé: {len(df)} articles\")\n",
    "print(f\"Colonnes: {list(df.columns)}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Nettoyage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Gestion des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes avant nettoyage:\n",
      "title                  0\n",
      "text                 631\n",
      "subject                0\n",
      "date                   0\n",
      "Detection              0\n",
      "detect_label           0\n",
      "text_length_chars      0\n",
      "text_length_words      0\n",
      "dtype: int64\n",
      "Nombre d'obs initial : 44898\n"
     ]
    }
   ],
   "source": [
    "# Etat avant nettoyage\n",
    "print(\"Valeurs manquantes avant nettoyage:\")\n",
    "print(df.apply(lambda x: x.astype(str).str.replace(r'\\s+', '', regex=True).eq('').sum()))\n",
    "\n",
    "initial_count = len(df)\n",
    "print(f\"Nombre d'obs initial : {initial_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres d'articles supprimés: 631\n"
     ]
    }
   ],
   "source": [
    "# Supprimer les lignes où text ou Detection ne contiennent que des espaces\n",
    "df = df[~df['text'].astype(str).str.replace(r'\\s+', '', regex=True).eq('')]\n",
    "\n",
    "print(f\"Nombres d'articles supprimés: {initial_count - len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Suppression des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons supprimés: 5623\n",
      "Articles restants: 38644\n"
     ]
    }
   ],
   "source": [
    "initial_count = len(df)\n",
    "df = df.drop_duplicates(subset=['text'], keep='first')\n",
    "\n",
    "print(f\"Doublons supprimés: {initial_count - len(df)}\")\n",
    "print(f\"Articles restants: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fusion des colonnes titre et texte\n",
    "\n",
    "Conformement à la méthodologie de l'article, nous combinons le titre et le texte pour fournir au modéle un contexte complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes 'title' et 'text' fusionnées dans 'combined_text'\n"
     ]
    }
   ],
   "source": [
    "df['combined_text'] = df['title'].astype(str) + \" \" + df['text'].astype(str)\n",
    "print(\"Colonnes 'title' et 'text' fusionnées dans 'combined_text'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 38644 entries, 0 to 44896\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   title              38644 non-null  object\n",
      " 1   text               38644 non-null  object\n",
      " 2   subject            38644 non-null  object\n",
      " 3   date               38644 non-null  object\n",
      " 4   Detection          38644 non-null  int64 \n",
      " 5   detect_label       38644 non-null  object\n",
      " 6   text_length_chars  38644 non-null  int64 \n",
      " 7   text_length_words  38644 non-null  int64 \n",
      " 8   combined_text      38644 non-null  object\n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation des données pour la division\n",
    "df_first = df.copy()\n",
    "# Ajout de l'identifiant\n",
    "df_first['id'] = range(1, len(df_first) + 1)\n",
    "df_first['label'] = df_first['Detection']\n",
    "\n",
    "# Reorganisation des colonnes\n",
    "cols = ['id', 'combined_text', 'label']\n",
    "df_first = df_first[cols]\n",
    "\n",
    "df_first['text'] = df_first['combined_text']\n",
    "\n",
    "cols = ['id', 'text', 'label']\n",
    "df_first = df_first[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 38644 entries, 0 to 44896\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      38644 non-null  int64 \n",
      " 1   text    38644 non-null  object\n",
      " 2   label   38644 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_first.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division du dataset:\n",
      "  Training set:   24,732 samples (64.0%)\n",
      "  Validation set: 6,183 samples (16.0%)\n",
      "  Test set:       7,729 samples (20.0%)\n"
     ]
    }
   ],
   "source": [
    "X = df_first[['id', 'text']]\n",
    "y = df_first['label']\n",
    "\n",
    "# Premiere division: 80% train+val, 20% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Deuxieme division: 80% train, 20% val (du train+val)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.20, random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(\"Division du dataset:\")\n",
    "print(f\"  Training set:   {len(X_train):,} samples ({len(X_train)/len(df_first)*100:.1f}%)\")\n",
    "print(f\"  Validation set: {len(X_val):,} samples ({len(X_val)/len(df_first)*100:.1f}%)\")\n",
    "print(f\"  Test set:       {len(X_test):,} samples ({len(X_test)/len(df_first)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets prepares:\n",
      "  train_df: (24732, 3)\n",
      "  val_df:   (6183, 3)\n",
      "  test_df:  (7729, 3)\n"
     ]
    }
   ],
   "source": [
    "# Creation des DataFrames interim\n",
    "train_df = pd.DataFrame({\n",
    "    'id': X_train['id'].values,\n",
    "    'text': X_train['text'].values,\n",
    "    'label': y_train.values\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'id': X_val['id'].values,\n",
    "    'text': X_val['text'].values,\n",
    "    'label': y_val.values\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'id': X_test['id'].values,\n",
    "    'text': X_test['text'].values,\n",
    "    'label': y_test.values\n",
    "})\n",
    "\n",
    "print(\"Datasets prepares:\")\n",
    "print(f\"  train_df: {train_df.shape}\")\n",
    "print(f\"  val_df:   {val_df.shape}\")\n",
    "print(f\"  test_df:  {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers sauvegardes:\n",
      "  - ../data/interim/train.csv\n",
      "  - ../data/interim/validation.csv\n",
      "  - ../data/interim/test.csv\n",
      "  - ../data/interim/full_interim.csv\n"
     ]
    }
   ],
   "source": [
    "# Creation du repertoire de sortie\n",
    "import os\n",
    "output_path = \"../data/interim/\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Sauvegarde des fichiers CSV\n",
    "train_df.to_csv(output_path + \"train.csv\", index=False)\n",
    "val_df.to_csv(output_path + \"validation.csv\", index=False)\n",
    "test_df.to_csv(output_path + \"test.csv\", index=False)\n",
    "\n",
    "# Sauvegarde du dataset complet preprocesse\n",
    "df.to_csv(output_path + \"full_interim.csv\", index=False)\n",
    "\n",
    "print(\"Fichiers sauvegardes:\")\n",
    "print(f\"  - {output_path}train.csv\")\n",
    "print(f\"  - {output_path}validation.csv\")\n",
    "print(f\"  - {output_path}test.csv\")\n",
    "print(f\"  - {output_path}full_interim.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing du texte\n",
    "\n",
    "### 5.1 Definition des fonctions de preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des outils NLP\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Conversion en minuscules\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Suppression des URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Suppression des mentions et hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Suppression de la ponctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Suppression des chiffres\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Suppression des espaces multiples\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Nettoyage initial\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Tokenisation\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Filtrage: stopwords et tokens courts\n",
    "    tokens = [token for token in tokens \n",
    "              if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # Lemmatisation\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Reconstruction\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Application du preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original (extrait):\n",
      "Ben Stein Calls Out 9th Circuit Court: Committed a ‘Coup d’état’ Against the Constitution 21st Century Wire says Ben Stein, reputable professor from, Pepperdine University (also of some Hollywood fame\n",
      "\n",
      "Texte preprocesse:\n",
      "ben stein call circuit court committed coup état constitution century wire say ben stein reputable professor pepperdine university also hollywood fame appearing show film ferris bueller day made provo\n"
     ]
    }
   ],
   "source": [
    "# Test sur un exemple\n",
    "sample_text = df['combined_text'].iloc[0]\n",
    "print(\"Texte original (extrait):\")\n",
    "print(sample_text[:200])\n",
    "print(\"\\nTexte preprocesse:\")\n",
    "print(preprocess_text(sample_text)[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du preprocessing sur tout le dataset\n",
    "df['processed_text'] = df['combined_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparaison avant/apres preprocessing:\n",
      "Original (2216 chars):\n",
      " Paul Ryan Responds To Dem’s Sit-In On Gun Control In The Most DISGUSTING Way (VIDEO) On Wednesday, Democrats took a powerful stance against the GOP s\n",
      "Preprocessed (1478 chars):\n",
      "paul ryan responds dem sitin gun control disgusting way video wednesday democrat took powerful stance gop refusal vote gun control measure staging sit\n"
     ]
    }
   ],
   "source": [
    "# Verification des resultats\n",
    "print(\"Comparaison avant/apres preprocessing:\")\n",
    "print(f\"Original ({len(df['combined_text'].iloc[5])} chars):\")\n",
    "print(df['combined_text'].iloc[5][:150])\n",
    "print(f\"Preprocessed ({len(df['processed_text'].iloc[5])} chars):\")\n",
    "print(df['processed_text'].iloc[5][:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Suppression des textes vides\n",
    "\n",
    "Après le preprocessing, certains textes peuvent devenir vides. Nous les supprimons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textes vides apres preprocessing: 5\n",
      "Articles supprimes: 5\n",
      "Articles restants: 38639\n"
     ]
    }
   ],
   "source": [
    "# Verification des textes vides\n",
    "empty_texts = (df['processed_text'].str.strip() == '').sum()\n",
    "print(f\"Textes vides apres preprocessing: {empty_texts}\")\n",
    "\n",
    "# Suppression si necessaire\n",
    "initial_count = len(df)\n",
    "df = df[df['processed_text'].str.strip() != '']\n",
    "\n",
    "print(f\"Articles supprimes: {initial_count - len(df)}\")\n",
    "print(f\"Articles restants: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ajout d'un identifiant unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>Detection</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>detect_label</th>\n",
       "      <th>text_length_chars</th>\n",
       "      <th>text_length_words</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ben stein call circuit court committed coup ét...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ben Stein Calls Out 9th Circuit Court: Committ...</td>\n",
       "      <td>21st Century Wire says Ben Stein, reputable pr...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>February 13, 2017</td>\n",
       "      <td>Fake</td>\n",
       "      <td>1028</td>\n",
       "      <td>171</td>\n",
       "      <td>Ben Stein Calls Out 9th Circuit Court: Committ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>trump drop steve bannon national security coun...</td>\n",
       "      <td>1</td>\n",
       "      <td>Trump drops Steve Bannon from National Securit...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>April 5, 2017</td>\n",
       "      <td>True</td>\n",
       "      <td>4820</td>\n",
       "      <td>771</td>\n",
       "      <td>Trump drops Steve Bannon from National Securit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>puerto rico expects lift jones act shipping re...</td>\n",
       "      <td>1</td>\n",
       "      <td>Puerto Rico expects U.S. to lift Jones Act shi...</td>\n",
       "      <td>(Reuters) - Puerto Rico Governor Ricardo Rosse...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>September 27, 2017</td>\n",
       "      <td>True</td>\n",
       "      <td>1848</td>\n",
       "      <td>304</td>\n",
       "      <td>Puerto Rico expects U.S. to lift Jones Act shi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>oops trump accidentally confirmed leaked israe...</td>\n",
       "      <td>0</td>\n",
       "      <td>OOPS: Trump Just Accidentally Confirmed He Le...</td>\n",
       "      <td>On Monday, Donald Trump once again embarrassed...</td>\n",
       "      <td>News</td>\n",
       "      <td>May 22, 2017</td>\n",
       "      <td>Fake</td>\n",
       "      <td>1244</td>\n",
       "      <td>183</td>\n",
       "      <td>OOPS: Trump Just Accidentally Confirmed He Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>donald trump head scotland reopen golf resort ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Donald Trump heads for Scotland to reopen a go...</td>\n",
       "      <td>GLASGOW, Scotland (Reuters) - Most U.S. presid...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>June 24, 2016</td>\n",
       "      <td>True</td>\n",
       "      <td>3137</td>\n",
       "      <td>529</td>\n",
       "      <td>Donald Trump heads for Scotland to reopen a go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                     processed_text  Detection  \\\n",
       "0   1  ben stein call circuit court committed coup ét...          0   \n",
       "1   2  trump drop steve bannon national security coun...          1   \n",
       "2   3  puerto rico expects lift jones act shipping re...          1   \n",
       "3   4  oops trump accidentally confirmed leaked israe...          0   \n",
       "4   5  donald trump head scotland reopen golf resort ...          1   \n",
       "\n",
       "                                               title  \\\n",
       "0  Ben Stein Calls Out 9th Circuit Court: Committ...   \n",
       "1  Trump drops Steve Bannon from National Securit...   \n",
       "2  Puerto Rico expects U.S. to lift Jones Act shi...   \n",
       "3   OOPS: Trump Just Accidentally Confirmed He Le...   \n",
       "4  Donald Trump heads for Scotland to reopen a go...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  21st Century Wire says Ben Stein, reputable pr...       US_News   \n",
       "1  WASHINGTON (Reuters) - U.S. President Donald T...  politicsNews   \n",
       "2  (Reuters) - Puerto Rico Governor Ricardo Rosse...  politicsNews   \n",
       "3  On Monday, Donald Trump once again embarrassed...          News   \n",
       "4  GLASGOW, Scotland (Reuters) - Most U.S. presid...  politicsNews   \n",
       "\n",
       "                  date detect_label  text_length_chars  text_length_words  \\\n",
       "0    February 13, 2017         Fake               1028                171   \n",
       "1       April 5, 2017          True               4820                771   \n",
       "2  September 27, 2017          True               1848                304   \n",
       "3         May 22, 2017         Fake               1244                183   \n",
       "4       June 24, 2016          True               3137                529   \n",
       "\n",
       "                                       combined_text  \n",
       "0  Ben Stein Calls Out 9th Circuit Court: Committ...  \n",
       "1  Trump drops Steve Bannon from National Securit...  \n",
       "2  Puerto Rico expects U.S. to lift Jones Act shi...  \n",
       "3   OOPS: Trump Just Accidentally Confirmed He Le...  \n",
       "4  Donald Trump heads for Scotland to reopen a go...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajout de l'identifiant\n",
    "df['id'] = range(1, len(df) + 1)\n",
    "\n",
    "# Reorganisation des colonnes\n",
    "cols = ['id', 'processed_text', 'Detection']\n",
    "additional_cols = [c for c in df.columns if c not in cols]\n",
    "df = df[cols + additional_cols]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Division du dataset\n",
    "\n",
    "Nous divisons le dataset selon le schema de l'article de reference:\n",
    "- **Training set**: 80%\n",
    "- **Validation set**: 20% du training (soit 16% du total)\n",
    "- **Test set**: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division du dataset:\n",
      "  Training set:   24,728 samples (64.0%)\n",
      "  Validation set: 6,183 samples (16.0%)\n",
      "  Test set:       7,728 samples (20.0%)\n"
     ]
    }
   ],
   "source": [
    "# Preparation des données pour la division\n",
    "X = df[['id', 'processed_text']]\n",
    "y = df['Detection']\n",
    "\n",
    "# Premiere division: 80% train+val, 20% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Deuxieme division: 80% train, 20% val (du train+val)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.20, random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(\"Division du dataset:\")\n",
    "print(f\"  Training set:   {len(X_train):,} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Validation set: {len(X_val):,} samples ({len(X_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Test set:       {len(X_test):,} samples ({len(X_test)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution des classes par split:\n",
      "Training set:\n",
      "Detection\n",
      "1    13562\n",
      "0    11166\n",
      "Name: count, dtype: int64\n",
      "Validation set:\n",
      "Detection\n",
      "1    3391\n",
      "0    2792\n",
      "Name: count, dtype: int64\n",
      "Test set:\n",
      "Detection\n",
      "1    4238\n",
      "0    3490\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verification de la distribution des classes dans chaque split\n",
    "print(\"Distribution des classes par split:\")\n",
    "print(\"Training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"Validation set:\")\n",
    "print(y_val.value_counts())\n",
    "print(\"Test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Preparation des datasets finaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets prepares:\n",
      "  train_df: (24728, 3)\n",
      "  val_df:   (6183, 3)\n",
      "  test_df:  (7728, 3)\n"
     ]
    }
   ],
   "source": [
    "# Creation des DataFrames finaux\n",
    "train_df = pd.DataFrame({\n",
    "    'id': X_train['id'].values,\n",
    "    'text': X_train['processed_text'].values,\n",
    "    'label': y_train.values\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'id': X_val['id'].values,\n",
    "    'text': X_val['processed_text'].values,\n",
    "    'label': y_val.values\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'id': X_test['id'].values,\n",
    "    'text': X_test['processed_text'].values,\n",
    "    'label': y_test.values\n",
    "})\n",
    "\n",
    "print(\"Datasets prepares:\")\n",
    "print(f\"  train_df: {train_df.shape}\")\n",
    "print(f\"  val_df:   {val_df.shape}\")\n",
    "print(f\"  test_df:  {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sauvegarde des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers sauvegardes:\n",
      "  - ../data/processed/train.csv\n",
      "  - ../data/processed/validation.csv\n",
      "  - ../data/processed/test.csv\n",
      "  - ../data/processed/full_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Creation du repertoire de sortie\n",
    "import os\n",
    "output_path = \"../data/processed/\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Sauvegarde des fichiers CSV\n",
    "train_df.to_csv(output_path + \"train.csv\", index=False)\n",
    "val_df.to_csv(output_path + \"validation.csv\", index=False)\n",
    "test_df.to_csv(output_path + \"test.csv\", index=False)\n",
    "\n",
    "# Sauvegarde du dataset complet preprocesse\n",
    "df.to_csv(output_path + \"full_processed.csv\", index=False)\n",
    "\n",
    "print(\"Fichiers sauvegardes:\")\n",
    "print(f\"  - {output_path}train.csv\")\n",
    "print(f\"  - {output_path}validation.csv\")\n",
    "print(f\"  - {output_path}test.csv\")\n",
    "print(f\"  - {output_path}full_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Résumé du preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESUME DU PREPROCESSING\n",
      "--------------------------------------------------------\n",
      "1. NETTOYAGE DES données\n",
      "   - Colonnes inutiles supprimees\n",
      "   - Valeurs manquantes gerees\n",
      "   - Doublons supprimes\n",
      "2. PREPROCESSING DU TEXTE\n",
      "   - Mise en minuscules\n",
      "   - Suppression URLs, mentions, hashtags\n",
      "   - Suppression ponctuation et chiffres\n",
      "   - Tokenisation\n",
      "   - Suppression des stopwords\n",
      "   - Lemmatisation\n",
      "4. DIVISION DU DATASET\n",
      "   - Training:   24728 samples\n",
      "   - Validation: 6183 samples\n",
      "   - Test:       7728 samples\n",
      "5. FICHIERS GENERES\n",
      "   - ../data/processed/train.csv\n",
      "   - ../data/processed/validation.csv\n",
      "   - ../data/processed/test.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"RESUME DU PREPROCESSING\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "print(\"1. NETTOYAGE DES données\")\n",
    "print(\"   - Colonnes inutiles supprimees\")\n",
    "print(\"   - Valeurs manquantes gerees\")\n",
    "print(\"   - Doublons supprimes\")\n",
    "\n",
    "print(\"2. PREPROCESSING DU TEXTE\")\n",
    "print(\"   - Mise en minuscules\")\n",
    "print(\"   - Suppression URLs, mentions, hashtags\")\n",
    "print(\"   - Suppression ponctuation et chiffres\")\n",
    "print(\"   - Tokenisation\")\n",
    "print(\"   - Suppression des stopwords\")\n",
    "print(\"   - Lemmatisation\")\n",
    "\n",
    "print(\"4. DIVISION DU DATASET\")\n",
    "print(f\"   - Training:   {len(train_df)} samples\")\n",
    "print(f\"   - Validation: {len(val_df)} samples\")\n",
    "print(f\"   - Test:       {len(test_df)} samples\")\n",
    "\n",
    "print(\"5. FICHIERS GENERES\")\n",
    "print(f\"   - {output_path}train.csv\")\n",
    "print(f\"   - {output_path}validation.csv\")\n",
    "print(f\"   - {output_path}test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prochaine étape\n",
    "\n",
    "Le notebook suivant (`fnd_03_modeling_classical.ipynb`) implementera les modeles classiques de classification (TF-IDF + Logistic Regression, Naive Bayes, SVM) comme baseline avant de passer aux modeles de deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- Roumeliotis, K.I., Tselikas, N.D., & Nasiopoulos, D.K. (2025). Fake News Detection and Classification: A Comparative Study of CNNs, LLMs, and NLP Models. *Future Internet*, 17, 28."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
