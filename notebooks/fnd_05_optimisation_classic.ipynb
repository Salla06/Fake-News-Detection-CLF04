{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fake News Detection - Phase 5: Optimisation et √âvaluation des Mod√®les Classiques\n",
        "\n",
        "### Objectifs de ce notebook\n",
        "\n",
        "**Objectif** : Optimiser les hyperparam√®tres des mod√®les classiques via GridSearchCV, √©valuer leurs performances sur les donn√©es de test, sauvegarder les mod√®les optimis√©s et comparer avec les mod√®les non optimis√©s.\n",
        "\n",
        "**Contenu** :\n",
        "- Chargement des donn√©es preprocess√©es\n",
        "- Optimisation des hyperparam√®tres (GridSearchCV)\n",
        "- √âvaluation sur les donn√©es de test\n",
        "- Sauvegarde des mod√®les (pickle) et performances (Excel)\n",
        "- Comparaison mod√®les optimis√©s vs non optimis√©s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 Importation des biblioth√®ques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Biblioth√®ques import√©es avec succ√®s\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# M√©triques\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, \n",
        "    f1_score, classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "# Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\" Biblioth√®ques import√©es avec succ√®s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 Chargement des donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:   24,728 samples\n",
            "Validation set: 6,183 samples\n",
            "Test set:       7,728 samples\n"
          ]
        }
      ],
      "source": [
        "# Chargement des datasets\n",
        "processed = \"../data/processed/\"\n",
        "classical = \"../models/classical/\"\n",
        "\n",
        "train_df = pd.read_csv(processed + \"train.csv\")\n",
        "val_df = pd.read_csv(processed + \"validation.csv\")\n",
        "test_df = pd.read_csv(processed + \"test.csv\")\n",
        "\n",
        "print(f\"Training set:   {len(train_df):,} samples\")\n",
        "print(f\"Validation set: {len(val_df):,} samples\")\n",
        "print(f\"Test set:       {len(test_df):,} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (24728,)\n",
            "y_train shape: (24728,)\n"
          ]
        }
      ],
      "source": [
        "# Preparation des donnees\n",
        "X_train = train_df['text'].values\n",
        "y_train = train_df['label'].values\n",
        "\n",
        "X_val = val_df['text'].values\n",
        "y_val = val_df['label'].values\n",
        "\n",
        "X_test = test_df['text'].values\n",
        "y_test = test_df['label'].values\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the vectorizer that was used during training\n",
        "with open('../models/classical/tfidf_vectorizer.pkl', 'rb') as f:  \n",
        "    vectorizer = pickle.load(f)\n",
        "\n",
        "X_train= vectorizer.transform(X_train)\n",
        "X_test= vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 Chargement des mod√®les de base (non optimis√©s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_results = {}\n",
        "\n",
        "models_baseline = {\n",
        "    'LinearSVC': \"../models/classical/linear_svm.pkl\",\n",
        "    'RandomForest': \"../models/classical/random_forest.pkl\",\n",
        "    'NaiveBayes': \"../models/classical/naive_bayes.pkl\",\n",
        "    'LogisticRegression': '../models/classical/logistic_regression.pkl'\n",
        "}\n",
        "\n",
        "# Now use the transformed data for predictions\n",
        "for name, path in models_baseline.items():\n",
        "    try:\n",
        "        with open(path, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        y_pred = model.predict(X_test)  # Use transformed data\n",
        "        baseline_results[name] = {\n",
        "            'accuracy': accuracy_score(y_test, y_pred),\n",
        "            'precision': precision_score(y_test, y_pred),\n",
        "            'recall': recall_score(y_test, y_pred),\n",
        "            'f1': f1_score(y_test, y_pred)\n",
        "        }\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Mod√®le {name} non trouv√©, sera ignor√© dans la comparaison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4 Optimisation des hyperparam√®tres\n",
        "\n",
        "Utilisation de GridSearchCV avec validation crois√©e 5-fold pour trouver les meilleurs hyperparam√®tres."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Linear SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimisation de Linear SVM...\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Meilleurs param√®tres: {'C': 10, 'loss': 'squared_hinge', 'max_iter': 1000}\n",
            "Meilleur score CV: 0.9956\n",
            " Mod√®le sauvegard√©\n"
          ]
        }
      ],
      "source": [
        "print(\"Optimisation de Linear SVM...\")\n",
        "\n",
        "# Grille d'hyperparam√®tres\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'loss': ['hinge', 'squared_hinge'],\n",
        "    'max_iter': [1000, 2000]\n",
        "}\n",
        "\n",
        "# GridSearch\n",
        "grid_svm = GridSearchCV(\n",
        "    LinearSVC(random_state=42, dual='auto'),\n",
        "    param_grid_svm,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_svm.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Meilleurs param√®tres: {grid_svm.best_params_}\")\n",
        "print(f\"Meilleur score CV: {grid_svm.best_score_:.4f}\")\n",
        "\n",
        "# Sauvegarde\n",
        "with open('../models/classic_ops/linear_svc_optimized.pkl', 'wb') as f:\n",
        "    pickle.dump(grid_svm.best_estimator_, f)\n",
        "\n",
        "print(\" Mod√®le sauvegard√©\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimisation de Random Forest...\n",
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "Meilleurs param√®tres: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n",
            "Meilleur score CV: 0.9962\n",
            " Mod√®le sauvegard√©\n"
          ]
        }
      ],
      "source": [
        "print(\"Optimisation de Random Forest...\")\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "grid_rf = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_grid_rf,\n",
        "    cv=5,\n",
        "    scoring = 'accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_rf.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Meilleurs param√®tres: {grid_rf.best_params_}\")\n",
        "print(f\"Meilleur score CV: {grid_rf.best_score_:.4f}\")\n",
        "\n",
        "with open('../models/classic_ops/random_forest_optimized.pkl', 'wb') as f:\n",
        "    pickle.dump(grid_rf.best_estimator_, f)\n",
        "\n",
        "print(\" Mod√®le sauvegard√©\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimisation de Naive Bayes...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "Meilleurs param√®tres: {'alpha': 0.1, 'fit_prior': False}\n",
            "Meilleur score CV: 0.9583\n",
            "Mod√®le sauvegard√©\n"
          ]
        }
      ],
      "source": [
        "print(\"Optimisation de Naive Bayes...\")\n",
        "\n",
        "param_grid_nb = {\n",
        "    'alpha': [0.1, 0.5, 1.0, 2.0],\n",
        "    'fit_prior': [True, False]\n",
        "}\n",
        "\n",
        "grid_nb = GridSearchCV(\n",
        "    MultinomialNB(),\n",
        "    param_grid_nb,\n",
        "    cv=5,\n",
        "    scoring = 'accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_nb.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Meilleurs param√®tres: {grid_nb.best_params_}\")\n",
        "print(f\"Meilleur score CV: {grid_nb.best_score_:.4f}\")\n",
        "\n",
        "with open('../models/classic_ops/naive_bayes_optimized.pkl', 'wb') as f:\n",
        "    pickle.dump(grid_nb.best_estimator_, f)\n",
        "\n",
        "print(\"Mod√®le sauvegard√©\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimisation de Logistic Regression...\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
          ]
        }
      ],
      "source": [
        "print(\"Optimisation de Logistic Regression...\")\n",
        "\n",
        "param_grid_lr = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['lbfgs', 'liblinear'],\n",
        "    'max_iter': [1000, 2000]\n",
        "}\n",
        "\n",
        "grid_lr = GridSearchCV(\n",
        "    LogisticRegression(random_state=42),\n",
        "    param_grid_lr,\n",
        "    cv=5,\n",
        "    scoring = 'accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_lr.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Meilleurs param√®tres: {grid_lr.best_params_}\")\n",
        "print(f\"Meilleur score CV: {grid_lr.best_score_:.4f}\")\n",
        "\n",
        "with open('../models/classic_ops/logistic_regression_optimized.pkl', 'wb') as f:\n",
        "    pickle.dump(grid_lr.best_estimator_, f)\n",
        "\n",
        "print(\" Mod√®le sauvegard√©\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5 √âvaluation sur les donn√©es de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionnaire pour stocker les mod√®les optimis√©s\n",
        "optimized_models = {\n",
        "    'LinearSVC': grid_svm.best_estimator_,\n",
        "    'RandomForest': grid_rf.best_estimator_,\n",
        "    'NaiveBayes': grid_nb.best_estimator_,\n",
        "    'LogisticRegression': grid_lr.best_estimator_\n",
        "}\n",
        "\n",
        "# √âvaluation de chaque mod√®le\n",
        "optimized_results = {}\n",
        "\n",
        "for name, model in optimized_models.items():\n",
        "    \n",
        "    # Pr√©dictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # M√©triques\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    \n",
        "    optimized_results[name] = {\n",
        "        'accuracy': acc,\n",
        "        'precision': prec,\n",
        "        'recall': rec,\n",
        "        'f1': f1,\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "    \n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall:    {rec:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    \n",
        "    print(\"Rapport de classification:\")\n",
        "    print(classification_report(y_test, y_pred, \n",
        "                                target_names=['True News', 'Fake News']))\n",
        "\n",
        "print(\"√âvaluation termin√©e\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6 Matrices de confusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, (name, results) in enumerate(optimized_results.items()):\n",
        "    cm = confusion_matrix(y_test, results['predictions'])\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=['True', 'Fake'],\n",
        "                yticklabels=['True', 'Fake'],\n",
        "                ax=axes[idx])\n",
        "    \n",
        "    axes[idx].set_title(f'{name} (Optimis√©)\\nF1-Score: {results[\"f1\"]:.4f}')\n",
        "    axes[idx].set_ylabel('Vraie Classe')\n",
        "    axes[idx].set_xlabel('Classe Pr√©dite')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/figures/confusion_matrices_optimized.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Matrices de confusion sauvegard√©es\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7 Comparaison mod√®les optimis√©s vs non optimis√©s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Trouver le meilleur mod√®le parmi tous (baseline + optimis√©s + deep learning)\n",
        "all_results = []\n",
        "\n",
        "# Mod√®les classiques\n",
        "for model_name in optimized_results.keys():\n",
        "    if model_name in baseline_results:\n",
        "        all_results.append({\n",
        "            'Mod√®le': f\"{model_name} (Baseline)\",\n",
        "            'F1-Score': baseline_results[model_name]['f1'],\n",
        "            'Accuracy': baseline_results[model_name]['accuracy'],\n",
        "            'Precision': baseline_results[model_name]['precision'],\n",
        "            'Recall': baseline_results[model_name]['recall']\n",
        "        })\n",
        "    \n",
        "    all_results.append({\n",
        "        'Mod√®le': f\"{model_name} (Optimis√©)\",\n",
        "        'F1-Score': optimized_results[model_name]['f1'],\n",
        "        'Accuracy': optimized_results[model_name]['accuracy'],\n",
        "        'Precision': optimized_results[model_name]['precision'],\n",
        "        'Recall': optimized_results[model_name]['recall']\n",
        "    })\n",
        "\n",
        "# Mod√®les Deep Learning\n",
        "deep_models = {\n",
        "    'CNN': '../models/deep/cnn_metrics.json',\n",
        "    'BiLSTM': '../models/deep/history_optimal.json'\n",
        "}\n",
        "\n",
        "for model_name, json_path in deep_models.items():\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            metrics = json.load(f)\n",
        "        \n",
        "        all_results.append({\n",
        "            'Mod√®le': model_name,\n",
        "            'F1-Score': metrics['f1_score'],\n",
        "            'Accuracy': metrics['accuracy'],\n",
        "            'Precision': metrics['precision'],\n",
        "            'Recall': metrics['recall']\n",
        "        })\n",
        "        print(f\"‚úì {model_name} charg√©\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ö† {json_path} non trouv√©\")\n",
        "    except KeyError as e:\n",
        "        print(f\"‚ö† Cl√© manquante dans {model_name}: {e}\")\n",
        "\n",
        "# Trier et afficher\n",
        "df_all = pd.DataFrame(all_results).sort_values('F1-Score', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLASSEMENT COMPLET DES MOD√àLES (par F1-Score)\")\n",
        "print(\"=\"*80)\n",
        "print(df_all.to_string(index=False))\n",
        "\n",
        "# Top 3\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üèÜ TOP 3 MEILLEURS MOD√àLES\")\n",
        "print(\"=\"*80)\n",
        "for i in range(min(3, len(df_all))):\n",
        "    model = df_all.iloc[i]\n",
        "    print(f\"\\n#{i+1} - {model['Mod√®le']}\")\n",
        "    print(f\"   F1-Score:  {model['F1-Score']:.4f}\")\n",
        "    print(f\"   Accuracy:  {model['Accuracy']:.4f}\")\n",
        "    print(f\"   Precision: {model['Precision']:.4f}\")\n",
        "    print(f\"   Recall:    {model['Recall']:.4f}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cr√©er un DataFrame de comparaison\n",
        "comparison_data = []\n",
        "\n",
        "for model_name in optimized_results.keys():\n",
        "    # Baseline\n",
        "    if model_name in baseline_results:\n",
        "        comparison_data.append({\n",
        "            'Mod√®le': model_name,\n",
        "            'Version': 'Baseline',\n",
        "            'Accuracy': baseline_results[model_name]['accuracy'],\n",
        "            'Precision': baseline_results[model_name]['precision'],\n",
        "            'Recall': baseline_results[model_name]['recall'],\n",
        "            'F1-Score': baseline_results[model_name]['f1']\n",
        "        })\n",
        "    \n",
        "    # Optimis√©\n",
        "    comparison_data.append({\n",
        "        'Mod√®le': model_name,\n",
        "        'Version': 'Optimis√©',\n",
        "        'Accuracy': optimized_results[model_name]['accuracy'],\n",
        "        'Precision': optimized_results[model_name]['precision'],\n",
        "        'Recall': optimized_results[model_name]['recall'],\n",
        "        'F1-Score': optimized_results[model_name]['f1']\n",
        "    })\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "print(\"\\nComparaison des performances:\")\n",
        "print(df_comparison.to_string(index=False))\n",
        "\n",
        "# Calcul des am√©liorations\n",
        "print(\"Am√©liorations apport√©es par l'optimisation:\")\n",
        "\n",
        "for model_name in optimized_results.keys():\n",
        "    if model_name in baseline_results:\n",
        "        f1_base = baseline_results[model_name]['f1']\n",
        "        f1_opt = optimized_results[model_name]['f1']\n",
        "        improvement = ((f1_opt - f1_base) / f1_base) * 100\n",
        "        \n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  F1 Baseline: {f1_base:.4f}\")\n",
        "        print(f\"  F1 Optimis√©: {f1_opt:.4f}\")\n",
        "        print(f\"  Am√©lioration: {improvement:+.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8 Visualisation comparative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graphique de comparaison\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    df_metric = df_comparison.pivot(index='Mod√®le', columns='Version', values=metric)\n",
        "    \n",
        "    df_metric.plot(kind='bar', ax=axes[idx], width=0.8)\n",
        "    axes[idx].set_title(f'Comparaison: {metric}', fontsize=14, fontweight='bold')\n",
        "    axes[idx].set_ylabel(metric, fontsize=12)\n",
        "    axes[idx].set_xlabel('Mod√®le', fontsize=12)\n",
        "    axes[idx].legend(title='Version', fontsize=10)\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "    axes[idx].set_ylim([0.9, 1.0])  # Ajuster selon vos r√©sultats\n",
        "    \n",
        "    # Ajouter les valeurs sur les barres\n",
        "    for container in axes[idx].containers:\n",
        "        axes[idx].bar_label(container, fmt='%.4f', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/figures/comparison_baseline_vs_optimized.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Graphiques de comparaison sauvegard√©s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.9 Identification du meilleur mod√®le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trouver le meilleur mod√®le parmi tous (baseline + optimis√©s)\n",
        "all_results = []\n",
        "\n",
        "for model_name in optimized_results.keys():\n",
        "    if model_name in baseline_results:\n",
        "        all_results.append({\n",
        "            'Mod√®le': f\"{model_name} (Baseline)\",\n",
        "            'F1-Score': baseline_results[model_name]['f1'],\n",
        "            'Accuracy': baseline_results[model_name]['accuracy']\n",
        "        })\n",
        "    \n",
        "    all_results.append({\n",
        "        'Mod√®le': f\"{model_name} (Optimis√©)\",\n",
        "        'F1-Score': optimized_results[model_name]['f1'],\n",
        "        'Accuracy': optimized_results[model_name]['accuracy']\n",
        "    })\n",
        "\n",
        "df_all = pd.DataFrame(all_results).sort_values('F1-Score', ascending=False)\n",
        "\n",
        "\n",
        "print(\"CLASSEMENT COMPLET DES MOD√àLES (par F1-Score)\")\n",
        "\n",
        "print(df_all.to_string(index=False))\n",
        "\n",
        "best_model = df_all.iloc[0]\n",
        "\n",
        "print(\" MEILLEUR MOD√àLE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Mod√®le: {best_model['Mod√®le']}\")\n",
        "print(f\"F1-Score: {best_model['F1-Score']:.4f}\")\n",
        "print(f\"Accuracy: {best_model['Accuracy']:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.10 Sauvegarde des r√©sultats dans Excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cr√©er un fichier Excel avec plusieurs feuilles\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "excel_file = f'results/model_performance_{timestamp}.xlsx'\n",
        "\n",
        "with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
        "    # Feuille 1: Comparaison compl√®te\n",
        "    df_comparison.to_excel(writer, sheet_name='Comparaison', index=False)\n",
        "    \n",
        "    # Feuille 2: Classement\n",
        "    df_all.to_excel(writer, sheet_name='Classement', index=False)\n",
        "    \n",
        "    # Feuille 3: Hyperparam√®tres optimaux\n",
        "    hyperparam_data = [\n",
        "        {'Mod√®le': 'LinearSVC', 'Param√®tres': str(grid_svm.best_params_)},\n",
        "        {'Mod√®le': 'RandomForest', 'Param√®tres': str(grid_rf.best_params_)},\n",
        "        {'Mod√®le': 'NaiveBayes', 'Param√®tres': str(grid_nb.best_params_)},\n",
        "        {'Mod√®le': 'LogisticRegression', 'Param√®tres': str(grid_lr.best_params_)}\n",
        "    ]\n",
        "    df_hyperparam = pd.DataFrame(hyperparam_data)\n",
        "    df_hyperparam.to_excel(writer, sheet_name='Hyperparam√®tres', index=False)\n",
        "    \n",
        "    # Feuille 4: Meilleur mod√®le\n",
        "    best_summary = pd.DataFrame([{\n",
        "        'Meilleur Mod√®le': best_model['Mod√®le'],\n",
        "        'F1-Score': best_model['F1-Score'],\n",
        "        'Accuracy': best_model['Accuracy'],\n",
        "        'Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    }])\n",
        "    best_summary.to_excel(writer, sheet_name='Meilleur Mod√®le', index=False)\n",
        "\n",
        "print(f\" R√©sultats sauvegard√©s dans: {excel_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.11 R√©sum√© final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"R√âSUM√â DE L'OPTIMISATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\" Fichiers g√©n√©r√©s:\")\n",
        "print(\"  - Mod√®les optimis√©s: models/*_optimized.pkl\")\n",
        "print(f\"  - Performances: {excel_file}\")\n",
        "print(\"  - Visualisations: figures/*.png\")\n",
        "\n",
        "print(\" Principaux r√©sultats:\")\n",
        "print(f\"  - Meilleur mod√®le: {best_model['Mod√®le']}\")\n",
        "print(f\"  - Performance: F1={best_model['F1-Score']:.4f}, Acc={best_model['Accuracy']:.4f}\")\n",
        "\n",
        "# Compter les am√©liorations\n",
        "improvements = 0\n",
        "for model_name in optimized_results.keys():\n",
        "    if model_name in baseline_results:\n",
        "        if optimized_results[model_name]['f1'] > baseline_results[model_name]['f1']:\n",
        "            improvements += 1\n",
        "\n",
        "print(f\" {improvements}/{len(optimized_results)} mod√®les am√©lior√©s par l'optimisation\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env_1_dta_scce",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
