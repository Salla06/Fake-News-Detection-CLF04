{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection - Phase 4: Deep Learning Models\n",
    "\n",
    "---\n",
    "\n",
    "### Objectifs de ce notebook\n",
    "\n",
    "Ce notebook implemente les modeles de deep learning pour la detection de fake news, conformement a la methodologie de l'article de reference (Roumeliotis et al., 2025). Ces modeles seront compares aux baselines classiques etablies precedemment.\n",
    "\n",
    "Modeles implementes:\n",
    "1. CNN (Convolutional Neural Network) pour le texte\n",
    "2. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "Selon l'article de reference, le CNN a atteint 58.6% d'accuracy tandis que les modeles bases sur les transformers (BERT, GPT) atteignent des performances superieures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dependances\n",
    "# !pip install tensorflow transformers torch scikit-learn pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPU disponible: []\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliotheques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "# Deep Learning - TensorFlow/Keras pour CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    ")\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Metriques\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflowNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached tensorflow-2.20.0-cp311-cp311-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (26.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (82.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in C:\\Users\\ndeye\\AppData\\Roaming\\Python\\Python311\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (1.78.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (3.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pillow in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.46.3)\n",
      "Requirement already satisfied: rich in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from keras>=3.10.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\Users\\ndeye\\anaconda3\\Lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Using cached tensorflow-2.20.0-cp311-cp311-win_amd64.whl (331.8 MB)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.20.0\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:   24,732 samples\n",
      "Validation set: 6,183 samples\n",
      "Test set:       7,729 samples\n"
     ]
    }
   ],
   "source": [
    "# Chargement des datasets preprocesses\n",
    "DATA_PATH = \"../data/interim/\"\n",
    "\n",
    "train_df = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "val_df = pd.read_csv(DATA_PATH + \"validation.csv\")\n",
    "test_df = pd.read_csv(DATA_PATH + \"test.csv\")\n",
    "\n",
    "print(f\"Training set:   {len(train_df):,} samples\")\n",
    "print(f\"Validation set: {len(val_df):,} samples\")\n",
    "print(f\"Test set:       {len(test_df):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donnees preparees.\n"
     ]
    }
   ],
   "source": [
    "# Preparation des donnees\n",
    "X_train = train_df['text'].values\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "X_val = val_df['text'].values\n",
    "y_val = val_df['label'].values\n",
    "\n",
    "X_test = test_df['text'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "print(f\"Donnees preparees.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN (Convolutional Neural Network)\n",
    "\n",
    "### 3.1 Theorie\n",
    "\n",
    "Les CNN appliques au texte utilisent des filtres de convolution pour capturer les patterns locaux (n-grams) dans les sequences de mots. L'architecture typique comprend:\n",
    "\n",
    "1. **Embedding Layer**: Transforme les indices de mots en vecteurs denses\n",
    "2. **Conv1D Layer**: Applique des filtres pour detecter des patterns\n",
    "3. **GlobalMaxPooling**: Extrait les features les plus saillantes\n",
    "4. **Dense Layers**: Classification finale\n",
    "\n",
    "Reference: Kaliyar et al. (2020) - FNDNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tokenisation et preparation des sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire: 116,489 mots\n",
      "Vocabulaire limite a: 10,000 mots\n"
     ]
    }
   ],
   "source": [
    "# Parametres de tokenisation\n",
    "MAX_VOCAB_SIZE = 10000    # Taille maximale du vocabulaire\n",
    "MAX_SEQUENCE_LENGTH = 512  # Longueur maximale des sequences (en tokens)\n",
    "EMBEDDING_DIM = 128        # Dimension des embeddings\n",
    "\n",
    "# Initialisation du tokenizer Keras\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=MAX_VOCAB_SIZE,\n",
    "    oov_token='<OOV>'  # Token pour les mots hors vocabulaire\n",
    ")\n",
    "\n",
    "# Entrainement du tokenizer sur les donnees d'entrainement\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "print(f\"Taille du vocabulaire: {len(tokenizer.word_index):,} mots\")\n",
    "print(f\"Vocabulaire limite a: {MAX_VOCAB_SIZE:,} mots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape des sequences:\n",
      "  Training:   (24732, 512)\n",
      "  Validation: (6183, 512)\n",
      "  Test:       (7729, 512)\n"
     ]
    }
   ],
   "source": [
    "# Conversion des textes en sequences d'indices\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding des sequences pour uniformiser la longueur\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "print(f\"Shape des sequences:\")\n",
    "print(f\"  Training:   {X_train_pad.shape}\")\n",
    "print(f\"  Validation: {X_val_pad.shape}\")\n",
    "print(f\"  Test:       {X_test_pad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Architecture du modele CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"\n",
    "    Cree un modele CNN pour la classification de texte.\n",
    "    \n",
    "    Architecture basee sur l'article de reference:\n",
    "    - Embedding -> Conv1D -> GlobalMaxPooling -> Dense -> Output\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Couche d'embedding\n",
    "        Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=max_length\n",
    "        ),\n",
    "        \n",
    "        # Couche de convolution 1D\n",
    "        Conv1D(\n",
    "            filters=128,\n",
    "            kernel_size=5,\n",
    "            activation='relu'\n",
    "        ),\n",
    "        \n",
    "        # Global Max Pooling\n",
    "        GlobalMaxPooling1D(),\n",
    "        \n",
    "        # Couches denses\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Couche de sortie\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Fonction de creation du modele CNN definie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation du modele\n",
    "cnn_model = create_cnn_model(\n",
    "    vocab_size=MAX_VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_length=MAX_SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "# Compilation du modele\n",
    "# Hyperparametres conformes a l'article: learning_rate=2e-5\n",
    "cnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=2e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Resume de l'architecture\n",
    "print(\"Architecture du modele CNN:\")\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Entrainement du CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'entrainement\n",
    "# Conformement a l'article: 3 epochs, batch_size=6\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 6\n",
    "\n",
    "# Callbacks pour l'entrainement\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=2,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: 2e-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement du modele\n",
    "print(\"Entrainement du CNN en cours...\")\n",
    "\n",
    "cnn_history = cnn_model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_data=(X_val_pad, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nEntrainement termine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Visualisation de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des courbes d'apprentissage\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(cnn_history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(cnn_history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_title('CNN - Evolution de la Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(cnn_history.history['accuracy'], label='Training Accuracy')\n",
    "axes[1].plot(cnn_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[1].set_title('CNN - Evolution de l\\'Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Evaluation du CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions sur le test set\n",
    "cnn_pred_proba = cnn_model.predict(X_test_pad, verbose=0)\n",
    "cnn_pred = (cnn_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calcul des metriques\n",
    "cnn_metrics = {\n",
    "    'model': 'CNN',\n",
    "    'accuracy': accuracy_score(y_test, cnn_pred),\n",
    "    'precision': precision_score(y_test, cnn_pred),\n",
    "    'recall': recall_score(y_test, cnn_pred),\n",
    "    'f1_score': f1_score(y_test, cnn_pred)\n",
    "}\n",
    "\n",
    "print(\"CNN - Resultats sur le TEST SET:\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in cnn_metrics.items():\n",
    "    if metric != 'model':\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, cnn_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Fake (0)', 'True (1)'],\n",
    "            yticklabels=['Fake (0)', 'True (1)'])\n",
    "plt.title('Matrice de confusion - CNN')\n",
    "plt.ylabel('Valeur reelle')\n",
    "plt.xlabel('Prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "### 4.1 Theorie\n",
    "\n",
    "BERT est un modele de langage pre-entraine base sur l'architecture Transformer. Ses avantages incluent:\n",
    "\n",
    "1. **Comprehension bidirectionnelle**: Considere le contexte des deux cotes d'un mot\n",
    "2. **Pre-entrainement massif**: Entraine sur de grands corpus (Wikipedia, BookCorpus)\n",
    "3. **Transfer learning**: Fine-tuning efficace pour des taches specifiques\n",
    "\n",
    "L'article de reference rapporte 97.5% d'accuracy avec BERT fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliotheques pour BERT\n",
    "try:\n",
    "    from transformers import (\n",
    "        BertTokenizer, BertForSequenceClassification,\n",
    "        Trainer, TrainingArguments\n",
    "    )\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    \n",
    "    BERT_AVAILABLE = True\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    BERT_AVAILABLE = False\n",
    "    print(\"Transformers ou PyTorch non disponible.\")\n",
    "    print(\"Installez avec: pip install transformers torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Preparation des donnees pour BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette section ne s'execute que si BERT est disponible\n",
    "\n",
    "# Chargement du tokenizer BERT\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Parametres\n",
    "BERT_MAX_LENGTH = 512  # Limite de BERT\n",
    "\n",
    "print(f\"Tokenizer BERT charge.\")\n",
    "print(f\"Longueur maximale: {BERT_MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe Dataset personnalisee pour PyTorch\n",
    "class FakeNewsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset PyTorch pour la classification de fake news avec BERT.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"Classe FakeNewsDataset definie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation des datasets\n",
    "train_dataset = FakeNewsDataset(X_train, y_train, bert_tokenizer, BERT_MAX_LENGTH)\n",
    "val_dataset = FakeNewsDataset(X_val, y_val, bert_tokenizer, BERT_MAX_LENGTH)\n",
    "test_dataset = FakeNewsDataset(X_test, y_test, bert_tokenizer, BERT_MAX_LENGTH)\n",
    "\n",
    "print(f\"Datasets crees:\")\n",
    "print(f\"  Training:   {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test:       {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Chargement et configuration du modele BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modele BERT pre-entraine\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2  # Classification binaire\n",
    ")\n",
    "\n",
    "print(f\"Modele BERT charge.\")\n",
    "print(f\"Nombre de parametres: {sum(p.numel() for p in bert_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Configuration de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments d'entrainement conformes a l'article\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./models/bert',\n",
    "    num_train_epochs=3,              # 3 epochs comme dans l'article\n",
    "    per_device_train_batch_size=6,   # Batch size = 6\n",
    "    per_device_eval_batch_size=6,\n",
    "    learning_rate=2e-5,              # Learning rate = 2e-5\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1'\n",
    ")\n",
    "\n",
    "print(\"Arguments d'entrainement configures.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour calculer les metriques\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'precision': precision_score(labels, predictions),\n",
    "        'recall': recall_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "print(\"Fonction compute_metrics definie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation du Trainer\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Trainer cree.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Entrainement du modele BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement\n",
    "print(\"Entrainement de BERT en cours...\")\n",
    "print(\"(Cette operation peut prendre plusieurs minutes sur GPU)\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nEntrainement termine.\")\n",
    "print(f\"Training loss finale: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Evaluation du modele BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation sur le test set\n",
    "print(\"Evaluation de BERT sur le test set...\")\n",
    "\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "bert_metrics = {\n",
    "    'model': 'BERT',\n",
    "    'accuracy': test_results['eval_accuracy'],\n",
    "    'precision': test_results['eval_precision'],\n",
    "    'recall': test_results['eval_recall'],\n",
    "    'f1_score': test_results['eval_f1']\n",
    "}\n",
    "\n",
    "print(\"\\nBERT - Resultats sur le TEST SET:\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in bert_metrics.items():\n",
    "    if metric != 'model':\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions pour la matrice de confusion\n",
    "bert_predictions = trainer.predict(test_dataset)\n",
    "bert_pred = np.argmax(bert_predictions.predictions, axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "cm_bert = confusion_matrix(y_test, bert_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_bert, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Fake (0)', 'True (1)'],\n",
    "            yticklabels=['Fake (0)', 'True (1)'])\n",
    "plt.title('Matrice de confusion - BERT')\n",
    "plt.ylabel('Valeur reelle')\n",
    "plt.xlabel('Prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaison des modeles Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation des resultats\n",
    "deep_results = pd.DataFrame([cnn_metrics, bert_metrics])\n",
    "deep_results = deep_results.set_index('model')\n",
    "\n",
    "print(\"Comparaison des modeles deep learning:\")\n",
    "print(\"=\"*60)\n",
    "print(deep_results.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparative\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.35\n",
    "\n",
    "# CNN\n",
    "cnn_values = deep_results.loc['CNN', metrics_to_plot].values\n",
    "ax.bar(x - width/2, cnn_values, width, label='CNN', color='#3498db')\n",
    "\n",
    "# BERT\n",
    "bert_values = deep_results.loc['BERT', metrics_to_plot].values\n",
    "ax.bar(x + width/2, bert_values, width, label='BERT', color='#27ae60')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Comparaison CNN vs BERT')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation des resultats\n",
    "\n",
    "Conformement aux observations de l'article de reference (Roumeliotis et al., 2025):\n",
    "\n",
    "1. Le CNN presente des performances limitees pour la classification de fake news, principalement en raison de sa difficulte a capturer les relations semantiques complexes\n",
    "\n",
    "2. BERT surpasse significativement le CNN grace a sa comprehension contextuelle bidirectionnelle et son pre-entrainement massif\n",
    "\n",
    "3. La difference de performance souligne l'importance des modeles bases sur les transformers pour les taches de NLP complexes comme la detection de fake news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sauvegarde des modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation des repertoires\n",
    "MODEL_PATH = \"./models/deep_learning/\"\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "# Sauvegarde du modele CNN\n",
    "cnn_model.save(MODEL_PATH + \"cnn_model.h5\")\n",
    "\n",
    "# Sauvegarde du tokenizer Keras\n",
    "with open(MODEL_PATH + \"keras_tokenizer.pkl\", 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(f\"Modele CNN sauvegarde: {MODEL_PATH}cnn_model.h5\")\n",
    "print(f\"Tokenizer sauvegarde: {MODEL_PATH}keras_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du modele BERT\n",
    "bert_model.save_pretrained(MODEL_PATH + \"bert_model/\")\n",
    "bert_tokenizer.save_pretrained(MODEL_PATH + \"bert_tokenizer/\")\n",
    "\n",
    "print(f\"Modele BERT sauvegarde: {MODEL_PATH}bert_model/\")\n",
    "print(f\"Tokenizer BERT sauvegarde: {MODEL_PATH}bert_tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des resultats\n",
    "deep_results.to_csv(MODEL_PATH + \"deep_learning_results.csv\")\n",
    "print(f\"\\nResultats sauvegardes: {MODEL_PATH}deep_learning_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation du resume\n",
    "print(\"=\"*60)\n",
    "print(\"RESUME - MODELES DEEP LEARNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. CNN (Convolutional Neural Network)\")\n",
    "print(f\"   - Architecture: Embedding + Conv1D + Dense\")\n",
    "print(f\"   - Parametres: ~{cnn_model.count_params():,}\")\n",
    "print(f\"   - F1-Score: {cnn_metrics['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\n2. BERT (bert-base-uncased)\")\n",
    "print(f\"   - Architecture: Transformer encoder\")\n",
    "print(f\"   - Parametres: ~110M\")\n",
    "print(f\"   - F1-Score: {bert_metrics['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\n3. COMPARAISON\")\n",
    "print(deep_results.round(4).to_string())\n",
    "\n",
    "print(\"\\n4. CONCLUSION\")\n",
    "print(\"   BERT surpasse significativement le CNN pour la detection de fake news,\")\n",
    "print(\"   confirmant les observations de l'article de reference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prochaine etape\n",
    "\n",
    "Le notebook suivant (`fnd_05_cross_validation.ipynb`) implementera la validation croisee pour une evaluation plus robuste des modeles.\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- Roumeliotis, K.I., Tselikas, N.D., & Nasiopoulos, D.K. (2025). Fake News Detection and Classification. *Future Internet*, 17, 28.\n",
    "- Kaliyar, R.K. et al. (2020). FNDNet - A Deep CNN for Fake News Detection. *Cognitive Systems Research*.\n",
    "- Devlin, J. et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers. *NAACL-HLT*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
