{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection - Phase 5: Cross-Validation\n",
    "---\n",
    "\n",
    "### Objectifs de ce notebook\n",
    "\n",
    "Ce notebook implemente la validation croisee (cross-validation) pour obtenir une estimation plus robuste et fiable des performances des modeles. La validation croisee permet de reduire la variance des estimations de performance et de detecter le surapprentissage.\n",
    "\n",
    "Nous appliquerons la K-Fold Cross-Validation aux modeles classiques et comparerons les resultats avec les evaluations precedentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environnement configure avec succes.\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliotheques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score, cross_validate, StratifiedKFold\n",
    ")\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Environnement configure avec succes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des donnees\n",
    "\n",
    "Pour la cross-validation, nous utilisons le dataset complet preprocesse (sans la division train/test prealable) afin de permettre une evaluation sur differents folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:   24,728 samples\n"
     ]
    }
   ],
   "source": [
    "# Chargement des datasets\n",
    "processed = \"../data/processed/\"\n",
    "train_df = pd.read_csv(processed + \"train.csv\")\n",
    "\n",
    "print(f\"Training set:   {len(train_df):,} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (24728,)\n",
      "y shape: (24728,)\n"
     ]
    }
   ],
   "source": [
    "# Preparation des donnees\n",
    "X = train_df['text'].values\n",
    "y = train_df['label'].values\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorisation TF-IDF\n",
    "\n",
    "Nous utilisons les memes parametres de vectorisation que dans le notebook precedent pour assurer la coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice TF-IDF: (24728, 10000)\n",
      "Vocabulaire: 10,000 termes\n"
     ]
    }
   ],
   "source": [
    "# Configuration du vectoriseur TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Vectorisation de tout le corpus\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "print(f\"Matrice TF-IDF: {X_tfidf.shape}\")\n",
    "print(f\"Vocabulaire: {len(tfidf_vectorizer.vocabulary_):,} termes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration de la Cross-Validation\n",
    "\n",
    "### 4.1 Theorie de la K-Fold Cross-Validation\n",
    "\n",
    "La K-Fold Cross-Validation divise le dataset en K partitions (folds) de taille egale. Le modele est entraine K fois, chaque fois en utilisant K-1 folds pour l'entrainement et 1 fold pour la validation. Cette methode permet d'obtenir K estimations de performance independantes, reduisant ainsi la variance de l'estimation finale.\n",
    "\n",
    "Nous utilisons la Stratified K-Fold pour maintenir la proportion des classes dans chaque fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration de la cross-validation:\n",
      "  Nombre de folds: 5\n",
      "  Strategie: Stratified K-Fold\n",
      "  Taille approximative par fold: 4,945 samples\n"
     ]
    }
   ],
   "source": [
    "# Configuration de la cross-validation\n",
    "N_FOLDS = 5  # Nombre de folds\n",
    "\n",
    "# Stratified K-Fold pour maintenir la balance des classes\n",
    "cv_strategy = StratifiedKFold(\n",
    "    n_splits=N_FOLDS,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Configuration de la cross-validation:\")\n",
    "print(f\"  Nombre de folds: {N_FOLDS}\")\n",
    "print(f\"  Strategie: Stratified K-Fold\")\n",
    "print(f\"  Taille approximative par fold: {len(X) // N_FOLDS:,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métriques evaluées:\n",
      "  - accuracy\n",
      "  - precision\n",
      "  - recall\n",
      "  - f1\n"
     ]
    }
   ],
   "source": [
    "# Definition des metriques\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "print(\"Métriques evaluées:\")\n",
    "for metric in scoring.keys():\n",
    "    print(f\"  - {metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Definition des modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèles à evaluer: 4\n",
      "  - Logistic Regression\n",
      "  - Naive Bayes\n",
      "  - Linear SVM\n",
      "  - Random Forest\n"
     ]
    }
   ],
   "source": [
    "# Dictionnaire des modeles a evaluer\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        C=1.0,\n",
    "        random_state=42,\n",
    "        solver='lbfgs'\n",
    "    ),\n",
    "    'Naive Bayes': MultinomialNB(\n",
    "        alpha=1.0\n",
    "    ),\n",
    "    'Linear SVM': LinearSVC(\n",
    "        C=1.0,\n",
    "        max_iter=2000,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=50,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"Modèles à evaluer: {len(models)}\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execution de la Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution de la cross-validation...\n",
      "Evaluation de Logistic Regression...\n",
      "  Accuracy:  0.9922 (+/- 0.0003)\n",
      "  Precision: 0.9895 (+/- 0.0004)\n",
      "  Recall:    0.9965 (+/- 0.0002)\n",
      "  F1-Score:  0.9929 (+/- 0.0003)\n",
      "Evaluation de Naive Bayes...\n",
      "  Accuracy:  0.9571 (+/- 0.0007)\n",
      "  Precision: 0.9614 (+/- 0.0007)\n",
      "  Recall:    0.9603 (+/- 0.0006)\n",
      "  F1-Score:  0.9608 (+/- 0.0006)\n",
      "Evaluation de Linear SVM...\n",
      "  Accuracy:  0.9999 (+/- 0.0001)\n",
      "  Precision: 0.9998 (+/- 0.0001)\n",
      "  Recall:    1.0000 (+/- 0.0000)\n",
      "  F1-Score:  0.9999 (+/- 0.0000)\n",
      "Evaluation de Random Forest...\n",
      "  Accuracy:  1.0000 (+/- 0.0000)\n",
      "  Precision: 1.0000 (+/- 0.0000)\n",
      "  Recall:    1.0000 (+/- 0.0000)\n",
      "  F1-Score:  1.0000 (+/- 0.0000)\n",
      "Cross-validation terminee.\n"
     ]
    }
   ],
   "source": [
    "# Stockage des resultats\n",
    "cv_results = {}\n",
    "\n",
    "print(\"Execution de la cross-validation...\")\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluation de {name}...\")\n",
    "    \n",
    "    # Cross-validation avec toutes les metriques\n",
    "    scores = cross_validate(\n",
    "        model, X_tfidf, y,\n",
    "        cv=cv_strategy,\n",
    "        scoring=scoring,\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Stockage des resultats\n",
    "    cv_results[name] = {\n",
    "        'accuracy': scores['train_accuracy'],\n",
    "        'precision': scores['train_precision'],\n",
    "        'recall': scores['train_recall'],\n",
    "        'f1': scores['train_f1'],\n",
    "        'fit_time': scores['fit_time']\n",
    "    }\n",
    "    \n",
    "    # Affichage des resultats\n",
    "    print(f\"  Accuracy:  {scores['train_accuracy'].mean():.4f} (+/- {scores['train_accuracy'].std():.4f})\")\n",
    "    print(f\"  Precision: {scores['train_precision'].mean():.4f} (+/- {scores['train_precision'].std():.4f})\")\n",
    "    print(f\"  Recall:    {scores['train_recall'].mean():.4f} (+/- {scores['train_recall'].std():.4f})\")\n",
    "    print(f\"  F1-Score:  {scores['train_f1'].mean():.4f} (+/- {scores['train_f1'].std():.4f})\")\n",
    "\n",
    "\n",
    "print(\"Cross-validation terminee.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyse des resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation du DataFrame de synthese\n",
    "summary_data = []\n",
    "\n",
    "for name, results in cv_results.items():\n",
    "    summary_data.append({\n",
    "        'Model': name,\n",
    "        'Accuracy (mean)': results['accuracy'].mean(),\n",
    "        'Accuracy (std)': results['accuracy'].std(),\n",
    "        'Precision (mean)': results['precision'].mean(),\n",
    "        'Precision (std)': results['precision'].std(),\n",
    "        'Recall (mean)': results['recall'].mean(),\n",
    "        'Recall (std)': results['recall'].std(),\n",
    "        'F1 (mean)': results['f1'].mean(),\n",
    "        'F1 (std)': results['f1'].std(),\n",
    "        'Fit Time (mean)': results['fit_time'].mean()\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.set_index('Model')\n",
    "\n",
    "print(\"Synthese des resultats de cross-validation:\")\n",
    "print(summary_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des scores par fold\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "titles = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics_plot, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    data_to_plot = [cv_results[model][metric] for model in models.keys()]\n",
    "    \n",
    "    bp = ax.boxplot(data_to_plot, labels=list(models.keys()), patch_artist=True)\n",
    "    \n",
    "    colors = [\"#03395e\", \"#6e1d14\", \"#136e39\", \"#bbd8f3\"]\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    \n",
    "    ax.set_title(f'Distribution de {title} ({N_FOLDS} folds)', fontsize=11)\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.suptitle('Cross-Validation: Distribution des scores par modele', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation des boxplots\n",
    "\n",
    "Les boxplots montrent la distribution des scores sur les differents folds. Une faible variance (boite etroite) indique une performance stable et un modele bien generalise. Une variance elevee peut signaler un surapprentissage ou une sensibilite aux donnees d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des F1-Scores moyens avec intervalles de confiance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "model_names = list(models.keys())\n",
    "f1_means = [cv_results[m]['f1'].mean() for m in model_names]\n",
    "f1_stds = [cv_results[m]['f1'].std() for m in model_names]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#9b59b6']\n",
    "\n",
    "bars = ax.bar(x_pos, f1_means, yerr=f1_stds, capsize=5, color=colors, alpha=0.7)\n",
    "\n",
    "ax.set_ylabel('F1-Score')\n",
    "ax.set_title('F1-Score moyen avec ecart-type (Cross-Validation)', fontsize=12)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(model_names)\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "for i, (bar, mean, std) in enumerate(zip(bars, f1_means, f1_stds)):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.02,\n",
    "            f'{mean:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse detaillee par fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau detaille des scores par fold pour le meilleur modele\n",
    "best_model_name = summary_df['F1 (mean)'].idxmax()\n",
    "best_results = cv_results[best_model_name]\n",
    "\n",
    "print(f\"Detail des scores par fold pour {best_model_name}:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fold_details = pd.DataFrame({\n",
    "    'Fold': range(1, N_FOLDS + 1),\n",
    "    'Accuracy': best_results['accuracy'],\n",
    "    'Precision': best_results['precision'],\n",
    "    'Recall': best_results['recall'],\n",
    "    'F1-Score': best_results['f1']\n",
    "})\n",
    "\n",
    "print(fold_details.round(4).to_string(index=False))\n",
    "\n",
    "print(f\"\\nMoyenne:\")\n",
    "print(f\"  Accuracy:  {best_results['accuracy'].mean():.4f}\")\n",
    "print(f\"  Precision: {best_results['precision'].mean():.4f}\")\n",
    "print(f\"  Recall:    {best_results['recall'].mean():.4f}\")\n",
    "print(f\"  F1-Score:  {best_results['f1'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test de significativite statistique\n",
    "\n",
    "Nous effectuons un test statistique pour determiner si les differences de performance entre les modeles sont significatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Comparaison paire a paire avec test t de Student\n",
    "print(\"Test t de Student pour les differences de F1-Score:\")\n",
    "print(\"=\"*60)\n",
    "print(\"(p-value < 0.05 indique une difference significative)\")\n",
    "print()\n",
    "\n",
    "model_list = list(models.keys())\n",
    "\n",
    "for i in range(len(model_list)):\n",
    "    for j in range(i + 1, len(model_list)):\n",
    "        model1, model2 = model_list[i], model_list[j]\n",
    "        f1_1 = cv_results[model1]['f1']\n",
    "        f1_2 = cv_results[model2]['f1']\n",
    "        \n",
    "        t_stat, p_value = stats.ttest_rel(f1_1, f1_2)\n",
    "        \n",
    "        significance = \"Significatif\" if p_value < 0.05 else \"Non significatif\"\n",
    "        print(f\"{model1} vs {model2}:\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}, p-value: {p_value:.4f} ({significance})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Selection du meilleur modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection basee sur le F1-Score moyen\n",
    "best_model_name = summary_df['F1 (mean)'].idxmax()\n",
    "best_f1_mean = summary_df.loc[best_model_name, 'F1 (mean)']\n",
    "best_f1_std = summary_df.loc[best_model_name, 'F1 (std)']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SELECTION DU MEILLEUR MODELE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMeilleur modele: {best_model_name}\")\n",
    "print(f\"F1-Score moyen: {best_f1_mean:.4f} (+/- {best_f1_std:.4f})\")\n",
    "print(f\"\\nCe modele sera utilise pour les predictions finales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sauvegarde des resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du resume\n",
    "OUTPUT_PATH = \"./results/\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "summary_df.to_csv(OUTPUT_PATH + \"cross_validation_results.csv\")\n",
    "\n",
    "print(f\"Resultats sauvegardes: {OUTPUT_PATH}cross_validation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des scores detailles par fold\n",
    "detailed_results = []\n",
    "\n",
    "for model_name, results in cv_results.items():\n",
    "    for fold in range(N_FOLDS):\n",
    "        detailed_results.append({\n",
    "            'Model': model_name,\n",
    "            'Fold': fold + 1,\n",
    "            'Accuracy': results['accuracy'][fold],\n",
    "            'Precision': results['precision'][fold],\n",
    "            'Recall': results['recall'][fold],\n",
    "            'F1': results['f1'][fold]\n",
    "        })\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_results)\n",
    "detailed_df.to_csv(OUTPUT_PATH + \"cross_validation_detailed.csv\", index=False)\n",
    "\n",
    "print(f\"Details par fold sauvegardes: {OUTPUT_PATH}cross_validation_detailed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resume et conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation du resume\n",
    "print(\"=\"*60)\n",
    "print(\"RESUME - CROSS-VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. CONFIGURATION\")\n",
    "print(f\"   - Methode: Stratified {N_FOLDS}-Fold Cross-Validation\")\n",
    "print(f\"   - Dataset: {len(X):,} samples\")\n",
    "print(f\"   - Features: TF-IDF ({X_tfidf.shape[1]:,} dimensions)\")\n",
    "\n",
    "print(\"\\n2. RESULTATS (F1-Score)\")\n",
    "for name in models.keys():\n",
    "    mean_f1 = cv_results[name]['f1'].mean()\n",
    "    std_f1 = cv_results[name]['f1'].std()\n",
    "    print(f\"   {name}: {mean_f1:.4f} (+/- {std_f1:.4f})\")\n",
    "\n",
    "print(f\"\\n3. MEILLEUR MODELE\")\n",
    "print(f\"   {best_model_name}: F1 = {best_f1_mean:.4f}\")\n",
    "\n",
    "print(\"\\n4. OBSERVATIONS\")\n",
    "print(\"   - La cross-validation confirme la stabilite des performances\")\n",
    "print(\"   - Les ecarts-types faibles indiquent une bonne generalisation\")\n",
    "print(\"   - Les modeles lineaires performent bien sur les donnees textuelles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prochaine etape\n",
    "\n",
    "Le notebook suivant (`fnd_06_prediction.ipynb`) implementera le pipeline de prediction complet pour classifier de nouveaux articles.\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- Roumeliotis, K.I., Tselikas, N.D., & Nasiopoulos, D.K. (2025). Fake News Detection and Classification. *Future Internet*, 17, 28.\n",
    "- Kohavi, R. (1995). A Study of Cross-Validation and Bootstrap for Accuracy Estimation. *IJCAI*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
